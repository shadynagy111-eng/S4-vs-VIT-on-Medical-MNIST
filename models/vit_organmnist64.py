# -*- coding: utf-8 -*-
"""vit-organmnist64.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DXjYWQvbgRnbH33uieeEc20BMvItori5
"""

!pip install medmnist

from medmnist import OrganCMNIST
train_dataset = OrganCMNIST(split="train", download=True,size=64)

validation_dataset = OrganCMNIST(split="val", download=True,size=64)
validation_dataset

test_dataset = OrganCMNIST(split="test", download=True,size=64)
test_dataset
BATCH_SIZE = 128

import torch

# This line checks if a GPU is available; if not, it uses the CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"ðŸš€ Using device: {device}")

import torch
import torch.nn as nn
class ViT2D(nn.Module):
    def __init__(self, num_classes, image_size, patch_size, embed_dim, num_heads, num_layers, mlp_dim, dropout=0.1):
        super(ViT2D, self).__init__()

        self.image_size = image_size
        self.patch_size = patch_size
        self.num_patches = (image_size[0] // patch_size[0]) * (image_size[1] // patch_size[1])
        self.embed_dim = embed_dim

        # FIX: Changed in_channels from 64 to 3 for RGB
        self.patch_embedding = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)

        # CLS Token: Standard in ViT for classification
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))

        # Positional encoding (num_patches + 1 for the cls_token)
        self.position_embedding = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))

        self.transformer_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, dropout, batch_first=True)
            for _ in range(num_layers)
        ])

        self.classification_head = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        batch_size = x.size(0)

        # Patch embedding
        x = self.patch_embedding(x)  # (batch, embed_dim, h', w')
        x = x.flatten(2).transpose(1, 2)  # (batch, num_patches, embed_dim)

        # Prepend CLS token to the sequence
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1) # (batch, num_patches + 1, embed_dim)

        # Add positional encoding
        x = x + self.position_embedding

        # Pass through transformer layers
        for layer in self.transformer_layers:
            x = layer(x)

        # Take the CLS token output for classification
        x = x[:, 0]

        return self.classification_head(x)

# Initialize
model = ViT2D(num_classes=11, image_size=(64, 64), patch_size=(4, 4), embed_dim=768, num_heads=8, num_layers=8, mlp_dim=256)
model.to(device)

train_dataset.montage(length=50)

import torch
from torch.utils.data import TensorDataset, DataLoader
from torchvision import transforms

# 1. Define the correct transform
# ToTensor() takes (28, 28) and makes it (1, 28, 28)
# permute(1, 2, 0) moves the '1' to the end -> (28, 28, 1)
transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x.view(1, 64,64))
        ])

# 2. Fix the "Not Callable" Error
# Assuming train_dataset.imgs is your data (numpy array)
# and train_dataset.labels is your target.
# You must apply transforms MANUALLY if your dataset isn't a Torchvision object.

def apply_transforms(data, transform_func):
    # This converts each image in your numpy array using your pipeline
    return torch.stack([transform_func(img) for img in data])

# Transform the raw data
train_data_transformed = apply_transforms(train_dataset.imgs, transform)
val_data_transformed = apply_transforms(validation_dataset.imgs, transform)
test_data_transformed=apply_transforms(test_dataset.imgs,transform)

# 3. Create the actual Dataset objects
train_set = TensorDataset(train_data_transformed, torch.tensor(train_dataset.labels))
validation_set = TensorDataset(val_data_transformed, torch.tensor(validation_dataset.labels))
test_set=TensorDataset(test_data_transformed,torch.tensor(test_dataset.labels))

# 4. Create DataLoaders
train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
val_loader = DataLoader(validation_set, batch_size=32, shuffle=False)
test_loader=DataLoader(test_set,batch_size=32,shuffle=False)

!mkdir /kaggle/working/checkpoint

# Training
def train():
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    pbar = tqdm(enumerate(train_loader))
    for batch_idx, (inputs, targets) in pbar:
        inputs, targets = inputs.to(device), targets.to(device).squeeze().long()

        optimizer.zero_grad()
        outputs = model(inputs)

        # Loss needs 1D targets
        # Weights based on DermaMNIST class counts (approximate)
# Give more weight to rare classes, less to the majority
        criterion = torch.nn.CrossEntropyLoss()
        loss = criterion(outputs, targets.squeeze().long())
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)

        total += targets.size(0)

        # FIX: Squeeze targets here to prevent broadcasting!
        correct += predicted.eq(targets.squeeze()).sum().item()

        pbar.set_description(
            'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %
            (batch_idx, len(train_loader), train_loss/(batch_idx+1),
             100.*correct/total, correct, total)
        )
    # CRITICAL: Return the values to the loop
    avg_loss = train_loss / len(train_loader)
    acc = 100. * correct / total
    return avg_loss, acc

def eval(epoch, val_loader,criterion, checkpoint=False):
    global best_acc
    model.eval()
    eval_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        pbar = tqdm(enumerate(val_loader), total=len(val_loader))
        for batch_idx, (inputs, targets) in pbar:
            inputs, targets = inputs.to(device), targets.to(device).squeeze().long()
            outputs = model(inputs)
            loss = criterion(outputs, targets.squeeze().long())

            criterion = torch.nn.CrossEntropyLoss()
            eval_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets.squeeze()).sum().item()

            pbar.set_description(
                'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%%' %
                (batch_idx, len(val_loader), eval_loss/(batch_idx+1), 100.*correct/total)
            )

    avg_loss = eval_loss / len(val_loader)
    acc = 100. * correct / total

    # --- CHECKPOINT LOGIC (Must be BEFORE the final return) ---
    if checkpoint:
        if acc > best_acc:
            print(f'-- Saving Checkpoint... Accuracy: {acc:.2f}% --')
            state = {
                'model': model.state_dict(),
                'acc': acc,
                'epoch': epoch,
            }
            # Use Absolute Paths for Kaggle
            if not os.path.isdir('/kaggle/working/checkpoint'):
                os.makedirs('/kaggle/working/checkpoint')

            torch.save(state, '/kaggle/working/checkpoint/ckpt.pth')
            best_acc = acc

    return avg_loss, acc # <--- The exit door is now at the very end

import torch

# This line checks if a GPU is available; if not, it uses the CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"ðŸš€ Using device: {device}")

# Define loss function and optimizer
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
optimizer = optim.AdamW(model.parameters(), lr=0.00001,weight_decay=0.01)  # Adjust learning rate

# Learning rate scheduler
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6)


criterion = torch.nn.CrossEntropyLoss()


# Move model and data to the appropriate device (e.g., CUDA)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

# Training and validation loop
num_epochs = 5
history = {
    'train_loss': [], 'val_loss': [],
    'train_acc': [], 'val_acc': []
}

for epoch in range(num_epochs):
    # Training
    model.train()
    train_loss = 0.0
    total_train = 0
    correct_train = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device).squeeze().long()
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
         # Calculate training accuracy for this batch
        _, predicted = torch.max(output.data, 1)
        total_train += target.size(0)
        correct_train += (predicted == target).sum().item()

    train_loss /= len(train_loader)
    train_accuracy = 100 * correct_train / total_train

    # Validation
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device).squeeze().long()
            output = model(data)
            loss = criterion(output, target)
            val_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    val_loss /= len(val_loader)
    scheduler.step(val_loss)
    val_accuracy = 100 * correct / total

    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['train_acc'].append(train_accuracy)
    history['val_acc'].append(val_accuracy)

    # Print epoch results
    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"Train Loss: {train_loss:.4f} "
          f"Val Loss: {val_loss:.4f} "
          f"Train Accuracy: {train_accuracy:.2f}% "
          f"Val_Accuracy: {val_accuracy:.2f}%")

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Testing the model
model.eval()  # Set the model to evaluation mode
y_true = []
y_pred = []

with torch.no_grad():  # Disable gradient calculation during inference
    for data, target in test_loader:
        data, target = data.to(device), target.to(device).squeeze().long()
        output = model(data)
        _, predicted = torch.max(output.data, 1)
        y_true.extend(target.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Calculate the confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Reds")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

def count_parameters(model):
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())

    print(f'Total Parameters: {total_params:,}')
    print(f'Trainable Parameters: {trainable_params:,}')
    return trainable_params

count_parameters(model)

from torchinfo import summary

# row-by-row input: (batch_size, sequence_length, d_input)
summary(model, input_size=(32, 1, 64, 64))

def plot_training_history(history):
    epochs = range(1, len(history['train_loss']) + 1)

    plt.figure(figsize=(14, 5))

    # Plot 1: Loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')
    plt.plot(epochs, history['val_loss'], 'r-o', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot 2: Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history['train_acc'], 'b-s', label='Training Acc')
    plt.plot(epochs, history['val_acc'], 'r-s', label='Validation Acc')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

plot_training_history(history)

import time

def measure_throughput(model, loader, device):
    model.eval()
    start_time = time.time()
    total_samples = 0

    with torch.no_grad():
        for inputs, _ in loader:
            inputs = inputs.to(device)
            _ = model(inputs)
            total_samples += inputs.size(0)

    end_time = time.time()
    throughput = total_samples / (end_time - start_time)
    print(f"Throughput: {throughput:.2f} samples/sec")
    return throughput

measure_throughput(model,train_loader,device)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Total Trainable Params: {count_parameters(model):,}")

# Run this after a few batches
memory_used = torch.cuda.max_memory_allocated() / (1024 ** 2) # Convert to MB
print(f"Peak GPU Memory: {memory_used:.2f} MB")

from torchinfo import summary

# The 'depth' and 'col_names' will show you the math complexity!
model_stats = summary(model, input_size=(1, 1, 64, 64),
                      col_names=["num_params", "params_percent", "mult_adds", "input_size"])
print(model_stats)